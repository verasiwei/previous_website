<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Classification]]></title>
    <url>%2F2019%2F06%2F16%2FClassification%2F</url>
    <content type="text"><![CDATA[Bagging (Random Forest)1.Bagging and BootstrapGiven a training set $X=x_{1},x_{2},…x_{n}$ with responses $Y=y_{1}, y_{2},.. y_{n}$, bagging repeatedly with K times, each time selects a subset of random samples with replacement from the training set and fits the algorithms like trees to these samples. Assuming we have trained a regression tree $f_{k}$ on $X_{k}$ and $Y_{k}$ where k=1…K. Predictions of unknown samples $X_{unknown}$ are based on the average of the predictions from all the trees $f_{k}$ on $X_{unknown}$. f'=\frac {1}{K} \sum_{k=1}^{K} f_{k}X_{unknown}Or as for classification trees, based on the majority votes of trees. The optimal number of trees K can be found through cross-validation or by out of bag error. 2.Decision TreeA decision treee is a tree where each node represents a feature, each branch represents a dicision and each leaf represents an outcome. Tree models where the outcome are discrete values are called classification trees and tress models where the outcome take continuous values are called regression trees. 3.Random ForestRandom forest selects a random subset of the features at each candidate split in the learning process. If one or a few features are very strong predictors for the response variable, these features will be selected in many of the K trees, which cause them to become correlated.I will use the package in python and house price data in Kaggle as an implemention example. 4.Implementation1234567891011121314151617181920212223242526import pandas as pdimport numpy as npfrom sklearn.ensemble import RandomForestClassifierfrom sklearn import ensemble, tree, linear_modelfrom sklearn.model_selection import train_test_split, cross_val_score# read data intrain = pd.read_csv(&quot;D:/work/kaggle/train.csv&quot;)test = pd.read_csv(&quot;D:/work/kaggle/test.csv&quot;)# skip the procedures of data manipulation and cleaningdef train_test(estimator, x_trn, x_tst, y_trn, y_tst): prediction_train = estimator.predict(x_trn) # Printing estimator print(estimator) # Printing train scores get_score(prediction_train, y_trn) prediction_test = estimator.predict(x_tst) # Printing test scores print(&quot;Test&quot;) get_score(prediction_test, y_tst)RM = ensemble.RandomForestRegressor(n_estimators=1000,random_state=0,criterion=&quot;mse&quot;,max_features=&quot;sqrt&quot;,max_depth=50).fit(x_train,y_train)train_test(RM,x_train,x_test,y_train,y_test) Boosting (Gradient Boosting)1.AlgorithmThe aim of the regression is to fit a model F(x) to predict the outcome $\hat{y}$ by minimizing the error. For example, in the situation of least-square regreesion setting, the aim of the regression is to fit a model F(x) to predict the outcome $\hat{y}$ by minimizing the mean squared error $\frac{1}{n} \sum_{i}(\hat{y_{i}}-y_{i})^2$. At each stage k, $1 \leq k \leq K$, the function at each stage is $F_{k}$. The gradient boosting improves on the model by constructing a new model to estimate the error: $F_{k+1}(x)=F_{k}(x)+h(x)$. Assuming $\hat{y}=F(x)$ and the loss function is $L(y,\hat{y})$. The error for all samples are $J=\sum_{i=1}^{n} L(y_i,\hat{y_i}$. Based on the chapter of algebra basic, the gradient of J is $\nabla J=\hat{y} -y$. Therefore the algorithm of gradient boosting is:1.initialize a model $F_{0}(x)$2.For k=1 to K:compute gradient r_{ki}=-\frac {\alpha L(y_{i}, F(x_{i}))}{\alpha F(x_{i})}$$. *Fit a function $h_{k}$* *compute $\gamma$* $$\gamma_{k}=argmin \sum_{i=1}^{n}L(y_{i},F_{k-1}(x_{i})+\gamma h_{k}(x_{i}))Update the model F_{k}(x)=F_{k-1}(x)+\gamma_{k}h_{k}(x)2.ImplementationI will use package in Python as an example. 12345from sklearn import ensemble, tree, linear_modelfrom sklearn.ensemble import GradientBoostingRegressorGBest = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features=&apos;sqrt&apos;, min_samples_leaf=15, min_samples_split=10, loss=&apos;huber&apos;).fit(x_train, y_train)]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Algebra Basic]]></title>
    <url>%2F2019%2F06%2F13%2FAlgebra_Basic%2F</url>
    <content type="text"><![CDATA[This is the review and cheatsheet of some basic linear algebra knowledge during my undergraduate study. 1. BasicDeterminantA scalar value that can be computed from elements of a square matrix. det(A)=|A|=\sum_{j=1}^{n}(-1)^{i+j}a_{i,j}M_{i,j}where $a_{ij}$ is the element of A and $M_{ij}$ is minor. Matrix InverseA square matrix M has an inverse that M is invertible if the determinant $|M| \neq 0$. A^{-1}A=IThe inverse properties are1.(A^{-1})^{-1}=A2.(AB)^{-1}=B^{-1}A^{-1}3.(A^{-1})^{T}=(A^{T})^{-1} Orthogonal MatricesA square matrix $A \in R^{nn}$ is orthogonal: A^{T}A=I=AA^{T}Matrix multiplication\(A \in R^{mn}\) and \(B \in R^{np}\) C=AB=\sum_{k=1}^{n}A_{ik}B_{kj}Matrix transpose (A^{T})^T=A(AB)^{T}=B^{T}A^{T}(A+B)^{T}=A^{T}+B^{T}TraceThe trace of a square matrix $A \in R^{nn}$ is denoted as tr(A), which is the sum of diagonal elements in the matrix:$tr(A)=\sum_{i=1}^{n}A_{ii}$. Properties are below:1.trA=trA^{T}2.tr(A+B)=trA+trB3.tr(tA)=t tr(A)4.tr(AB)=tr(BA)5.tr(ABC)=tr(BCA)=tr(CAB) RankThe column rank of a matrix $A \in R^{mn}$ is the size of the largest subset of columns of A that constitute a linearly independent set. The same definition as for row rank. For any matrix, the column rank is equal to the row rank. Both are denoted as rank(A). Properties are below:1.rank(A) \leq min(m,n)2.rank(A)=rank(A^{n}3.rank(AB) \leq min(rank(A), rank(B))4.rank(A+B) \leq rank(A)+rank(B) Eigenvalue and EigenvectorsGiven a square matrix A, \(\lambda\) is an eigenvalue and x is the corresponding eigenvector if Ax=\lambda x, x \neq 0which equals to (\lambda I-A)x=0The properties are1.tr(A)=\sum_{i=1}^{n} \lambda_{i}2.|A|=\prod{i=1}^n \lambda_{i}3.The rank of A is equal to the number of non-zero eigenvalues of A4.If A is non-singular then $1/\lambda_{i}$ is an eigenvalue of $A^{-1}$ with associated eigenvector.5.The eigenvalues of a diagonal matrix are just the diagonal entries. The gradient$f: R^{mn} \to R$ is a function that input a matrix A and returns a value. The gradient of f is the matrix of partial derivatives. \left[ \begin{matrix} \alpha f(A)/(\alpha A_{11}) & \alpha f(A)/(\alpha A_{12}) & ... \alpha f(A)/(\alpha A_{1n}) \\\\ \alpha f(A)/(\alpha A_{21}) & \alpha f(A)/(\alpha A_{22}) & ... \alpha f(A)/(\alpha A_{2n}) \\\\ ...... \\\\ \alpha f(A)/(\alpha A_{m1}) & \alpha f(A)/(\alpha A_{m2}) & ... \alpha f(A)/(\alpha A_{mn}) \\\\ \end{matrix} \right]2.Other definitions and calculationsLaplace Matrix (simple graph)Given a simple graph G with n vertices, its Laplace Matrix \(L_{nn}\) is defined as: L=D-A, where D is the degree matrix and A is the adjacency matrix. D is a diagonal matrix which includes the information about the degree of each vertex. A is the adjacency matrix which only includes 1 and 0 since G is a simple graph and the diagonal are all 0.symmetrix normalized laplacian L^{sym}=D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}Singular value decompositionAssume $A \in R^{mn}$ and all elements in M belongs to real or plural values. There exists a decomposition that A=U \sum V^{T}where U and V are orthogonal that $U^{T}U=I_{mm}$ and $V^{T}V=I_{nn}$. A^{T}A=V (\sum)^{T} \sum V^{T}AA^{T}=U \sum (\sum)^{T} U^{T}Eigen decomposition$A \in R^{nn}$ is a square matrix with n linear independent eigenvectors $q_{i}$(i=1…n). A can be factorized as A=Q\Lambda Q^{-1}where Q is the square n by n matrix with ith column is the eigenvector of A, and $\Lambda$ is the diagonal matrix with diagonal elements are the corresponding eigenvalues.]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mixed Effect Model]]></title>
    <url>%2F2019%2F05%2F01%2FMixed-Effect-Model%2F</url>
    <content type="text"><![CDATA[Assuming there is a continuous outcome measure y, and the interest is to examine the relationship between y and a continuous variable called $x_{1}$ that the model is something like this $y~\beta_{0}+\beta_{1}x_{1}+\epsilon$. And assuming we also have age and sex fixed effects in the model, so the model is like this $y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}age+\beta_{3}sex+\epsilon$. But the situation is that for each subject, we took multiple measures at different timepoints that each subject have multiple responses at different timepoints, so multiple responses from the same subject are not independent. We need to add random effect in the regression model to deal with this situation. Each subject might have different “baseline” y value that subject 1 might have a mean y value1 and subject 2 might have a mean y value2 across three timepoints. Therefore we need to include subjects as one random effect. $y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}age+\beta_{3}sex+\gamma_{1}subject+\epsilon$]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Self-Study-SQL-Notebook]]></title>
    <url>%2F2019%2F03%2F27%2FSelf-Study-SQL-Notebook%2F</url>
    <content type="text"><![CDATA[SQL is a very simple modular query language. I have a bad memory that sometimes I really need a cheatsheet ~_~ PostgreSQL FundamentalSELECT1) select all columnsSELECT * FROM table; 2) select some columnsSELECT column1,column2,… FROM table; 3) select distinct statement in a tableSELECT DISTINCT column1,column2,…FROM table; 4) select where statementSELECT column1,column2,… FROM tableWHERE column1 &lt;&gt;= m AND column2 &gt;= n ; COUNT1) count does not consider NULLSELECT COUNT(DISTINCT column) FROM table; LIMIT1) specify how many rows to selectSELECT * FROM tableLIMIT 5; ORDER BYSELECT column_1,column2 FROM table_nameORDER BY column_1 ASC,column_2 DESC; BETWEENSELECT column_1,column2 FROM table_nameWHERE column_1 BETWEEN m AND n; INSELECT column_1, column2 FROM table_nameWHERE column_1 IN (1,2)ORDER BY column_2 DESC; LIKE1)SELECT column_1,column_2 FROM table_nameWHERE column_1 LIKE ‘xxx%’; 2)sensitive to case charactersSELECT column_1,column_2 FROM table_nameWHERE column_1 ILIKE ‘%xxx’; 3)SELECT column_1,column_2 FROM table_nameWHERE column_1 NOT LIKE ‘%xxx%’; GROUP BY StatementsMIN, MAX, AVG, SUM1)SELECT ROUND(AVG(column_1),m) FROM table_name; 2)SELECT ROUND(MIN(column_1),m) FROM table_name; 3)SELECT ROUND(MAX(column_1),m) FROM table_name; 4)SELECT ROUND(SUM(column_1),m) FROM table_name; GROUP BY1)SELECT column_1, SUM(column_2) FROM table_nameGROUP BY column_1ORDER BY SUM(column_2) DESC; 2)SELECT column_1, COUNT(*) FROM table_nameGROUP BY column_1; 3)SELECT column_1, COUNT(column_1), SUM(column_1)FROM table_nameGROUP BY column_1; HAVING1) The condition applies to the group rows created by the GROUP BY, but WHERE applies before GROUP BYSELECT column_1, aggregate_function(column_2)FROM table_nameWHERE column_1 IN (“”,””,””)GROUP BY column_1HAVING condition; JOINSAS1)SELECT column_1, SUM(column_2) AS nameFROM table_nameGROUP BY column_1; Inner Join1)SELECET column_1, column_2, column_3, column_4FROM table1_nameINNER JOIN table2_name ON table1_name.column_1 = table2_name.column_1; 2)SELECT column_1, column_2 AS name_1FROM table1_nameJOIN table2_name AS name_2 ON name_2.column_1=table1_name.column_1; FULL OUTER JOIN1) The set of all records in table 1 and table 2, the missingness will contain NULLSELECT * FROM table1_nameFULL OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2 LEFT OUTER JOIN1) A complete set of records from table 1 and the matching records in table 2, the missingness of right table will be NULLSELECT * FROM table1_nameLEFT OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2 2) the records only in table 1 but not in table 2SELECT * FROM table1_nameLEFT OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2WHERE table1_name.id IS null FULL OUTER JOIN1) the records unique to table 1 and table 2SELECT * FROM table1_nameFULL OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2WHERE table1.id IS null OR table2.id IS null UNION1) all records in table 1 and table 2 even some are sameSELECT column_1,column_2FROM table1_nameUNION ALLSELECT column_1,column_2FROM table2_name; 2) removes all duplicate rowsSELECT column_1,column_2FROM table1_nameUNIONSELECT column_1,column_2FROM table2_name; Timestamps and Extract1) Extract daySELECT extract(day from column_1) FROM table_name;2) Extract monthSELECT extract(month from column_1) FROM table_name; Mathematical Functions1) additionSELECT column_1+column_2 AS newcolumnFROM table_name; 2) multiplySELECT column_1*column_2 AS newcolumnFROM table_name; 3) divitionSELECT column_1/column_2 AS newcolumnFROM table_name; String Functions1)SELECT column_1 || ‘ ‘ || column_2 FROM table_name; 2)SELECT lower(column_1) FROM table_name; Subquery1)SELECT column_1, column_2, column_3 FROM table_nameWHEREcolumn_3 &gt; (SELECT AVG(column_3) FROM table_name); 2)SELECT table1_name.column_1FROM table1_nameINNER JOIN table2_name ON table1_name.column_1 = table2_name.column_1WHEREcolumn_2 BETWEEN ‘’ AND ‘’; Self Join1) SubquerySELECT column_1 FROM table1_nameWHERE column_2 IN(SELECT column_2 FROM table2_name)WHERE column_1 = “ “) 2) Self joinSELECT table1_name.column_1FROM table1_name AS newname1, table1_name AS newname2WHEREnewname1.column_2 = newname2.column_2AND newname2.column_1=” “; Creating Databases and Tables1) Data typescharacter:char(n)integers: int(n)real: float(n)numeric: numeric(n) 2) Create tableCREATE TABLE table_name ( column_name data_type column_constraint, table_constraint)INHERITS existing_table_name; 3) INSERTINSERT INTO table(column1,column2)VALUES (value1,value2),VALUES (value1,value2); Insert from another tableINSERT INTO tableSELECT column1,column2,…FROM another_tableWHERE condition; 4) UPDATEUPDATE tableSET column1 = value1, column2 = value2,…WHERE conditionRETURNING column1, column2,…; 5) DELETEDELETE FROM tableWHERE condition 6) Alter TableALTER TABLE table_name DROP COLUMN column_1;ALTER TABLE table_name ADD COLUMN column_1 boolean;ALTER TABLE table_name RENAME COLUMN column1 TO new_column1_name; 7) Drop TableDROP TABLE IF EXISTS table_name RESTRICT; 8) Check ConstraintCREATE TABLE table_name( column_1 serial PRIMARY KEY, column_2 VARCHAR(50), column_3 DATE CHECK(column_3 &gt; ‘1900-01-01’), column_4 DATE CHECK(column_4 &gt; column_3), column_5 integer CHECK(column_5&gt;0));]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Self-Study AWS Notebook]]></title>
    <url>%2F2019%2F03%2F12%2FSelf-Study-AWS-Notebook%2F</url>
    <content type="text"><![CDATA[Updating……They are all modular things, a cheatsheet to save time when I cannot remember something StorageAWS S3Block storage: stores data organized as an array of unrelated blocksFile storage: stores data in a hierarchy of files and foldersObject storage: stores data as objects which consist of data, metadata and identifier Buckets: Create a bucket and store data/Pay only what you use/Region basedObjects: Objects stored in buckets(image, word…) Object metadta=Name | ValueObject storage classes: S3 standard; S3 intelligent-tiering; S3 standard-IA; S3 One Zone-IA; S3 Glacier 12345#insert your AWS access key ID and secret access keyD:\&gt; aws configureD:\&gt; aws s3api list-bucketsD:\&gt; aws s3 ls s3://D:\&gt; aws s3api head-object --bucket bucketname --key objectname S3 access permission: how to add permission of another account? https://docs.aws.amazon.com/quicksight/latest/user/using-s3-files-in-another-aws-account.html12345678910111213#Object#abort multipart uploads3:AbortMultipartUpload#delete objects3:DeleteObject#get object, head object, select object contents3:GetObject#Bucket#get public access blocks3:GetAccountPublicAccessBlock#put public accessblock, delete public access blocks3:PutAccountPublicAccessBlock ComputeAWS EC2EC2 is the AWS to create and run virtual machines in the cloud, the VM is called “instances”. To use an SSH client to connect to an linux instance1ssh -i /path/MyKeyPair.pem ec2-user@IP_address Move data to and from S3 and EC2 instance12345ec2-user@IPaddress $ wget https://bucket.s3.amazonaws.com/path-to-fileorec2-user@IPaddress $ aws s3 cp s3://bucket/path-to-file filenameor download an entire S3 bucket to a directory on the instanceec2-user@IPaddress $ aws s3 sync s3://bucket directory_on_instance AnalyticsAmazon QuickSight1) creating a data set using S3 files2) creating a data set using Amazon Athena Data: https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Rent Apartment in Nashville]]></title>
    <url>%2F2019%2F03%2F06%2FRent%20Apartment%20in%20Nashville%2F</url>
    <content type="text"><![CDATA[Rent Apartment in Nashvilleprerequisite: SAFE! SAFE! SAFE! Gated community! Gated community! Gated community! better: In unit laundry; Top floors Next to Vanderbilt: Apartment 1) West end village: 2) Americana Apartments Condo(Personal Landlord) 1) Continental Condo]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cheatsheet Shell Scripts]]></title>
    <url>%2F2019%2F01%2F04%2FCheatsheet-Shell-Scripts%2F</url>
    <content type="text"><![CDATA[Something useful for shell scripts/gawk scripts/git commands/aws commandscreate public ssh key12$ ssh-keygen -t rsa -C &quot;siwei&quot;$ cat ~/.ssh/id_rsa.pub connect to the remote server and modify the files on the server remotely123#download rmate firstly$ ssh -R 52698:127.0.0.1:52698 username@server $ rmate -p 52698 filename change group of files12#change all files under this folder$ chgrp -R group file change permission12345678#have a look of the permission of a folder/file$ getfacl &quot;file&quot;#change the permission of a user read/write/execute$ setfacl -m u:userID:rwx &quot;file&quot;#change the permission of a group read/write/execute $ setfacl -m g:groupID:rwx &quot;file&quot; deal with gzipped files12345678#have a look of compressed files$ zcat file | head#uncompress files$ gunzip -c file &gt; /directory/file#compress files$ gzip -c file &gt; /directory/file loop123$for i in `seq 1 22`;dogawk -v chr=$&#123;i&#125; &apos;$5==&quot;SNP&quot; &#123;print chr&quot; &quot;$2&#125;&apos; &gt;&gt; filedone ifelse1234567$ if [&quot;answer&quot; == &quot;local&quot;]; thenfor i in `seq 1 22`;doecho &quot;hello world&quot; &gt;&gt; file1doneelseecho &quot;hello world&quot; &gt;&gt; file2fi git commands1234567#one local repository to multiple remote repositories$ git remote add both repository1$ git remote remote set-url --add --push both repository1$ git remote set-url --add --push both repository2$ git add .$ git commit -m first_commit$ git push both aws commandsReference: from AWS tutorialAmazon S3 is a repository for internet data. It is designed to store and retrieve any amount of data from Amazon EC2. Amazon EC2 uses Amazon S3 for storing Amazon Machine Images(AMIs), users use AMI for launching EC2 instances. 1234#to get started$ aws configure#list the contents in S3$ aws s3api list-buckets]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GWAS Pipeline]]></title>
    <url>%2F2018%2F12%2F31%2FGWAS-Pipeline%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[灯影牛肉]]></title>
    <url>%2F2018%2F12%2F16%2F%E7%81%AF%E5%BD%B1%E7%89%9B%E8%82%89%2F</url>
    <content type="text"><![CDATA[灯影牛肉1.买bottom round或者top round部位,不要买牛腱,因为太筋道不好撕，附上牛身上各部分的图片2.用料酒泡,再用水煮一下去除血水3.instant pot里放入肉,放大料,八角,桂皮,酱油,一包卤料,加水没过牛肉4.按meatstew按钮,时间调成一小时5.完事了卤牛肉6.然后晾干牛肉,撕成丝7.锅里放大把的油,五香粉,辣椒粉,大把的糖,倒入牛肉丝翻炒8.all done!至于我为什么会做这么复杂又耗时间的一道菜,原因是本来我想做卤牛肉的,没成想没买到牛腱,做出来的卤牛肉切不成片,口感又老,所以改成了这道菜]]></content>
      <categories>
        <category>cook menu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[deign_analysis_interpretation_GWAS]]></title>
    <url>%2F2018%2F11%2F19%2Fdeign-analysis-interpretation-GWAS%2F</url>
    <content type="text"><![CDATA[Notes for the book: Design, Analysis, and Interpretation of Genome-Wide Association Scans Introduction:Charles Darwin’s workLed most directly to the field of genetics, which is the theory of evolution: 1) More individuals are produced each generation that survive. 2) Phenotypic variation exists among individuals and the variation is heritable. 3) Those individuals with heriatble traits better suited to the environment will survive. 4) When reproductive isolation occurs new species will form [Wiki] Fundamental to the emergence of modern genetics was the discovery by Gregor Mendel:1) Law of Segregation: When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. 2) Law of Independent Assortment: Each pair of alleles segregates independently of the other pairs of alleles during gamete formation. 3) Law of Dominance: If the two alleles of an inherited pair differ, then one determines the organism’s appearance and is called the dominant allele. [Wiki] Organization of Chromosomes:22 autosomes and 2 sex chromosomes. The Centromere (which I circled in the example picture[Wiki]) divides the chromosome into two arms, the shorter arm is the p arm and the longer is the q arm. Example: 1q22 is the 2th band of the 2nd region of the long arm of chromosome 1. Organization of DNA:Each chromosome is made up of two strands of DNA, each strand has a deoxyribose backbone, each sugar having a 3’ carbon linked by a phosphate group to the 5’ carbon. The strand issue is the problem when compare SNP data from different genotyping platforms. If one platform uses probes for one strand and another platform uses the reverse probse, it will be read differently. So define “plus” or “forward” strand for a given chromosome as the strand of DNA for which the 5’ end is the nearest to the centromere, another definition is moving from the 5’ to the 3’ end moves in increasing base position order. The “minor” or “reverse” strand is the complementary strand to plus strand. Example in the picture: Types of Genetic Variation1) An SNV consists of a single base variation at a specific position on a given strand of DNA with the change measured relative to some existing consensus sequence. If an SNV presents at least 1%, it will be denoted as SNP.2) Insertions/Deletions3) Larger Structured Variants: including deletions, insertions, and duplications of segments of DNA that are 1kb or greater in extent are termed copy number variants or CNVs. Other types of structural variants include inversions and translocations.4) Exonic Variation: Mendelian traits are most often due to protein changes in specific genes.5) Non-exonic SNPs and Disease6) SNP Haplotypes: a combination of from several to many SNPs on the same chromosome segment of DNA. Basic Genomics:dbSNP: SNPs and other variants are reported.UCSC Genome Browser: tools and datasets compiling sequence information for many genome. The blat tool maps sequences to genome location and strand is also available?1000 Genomes Project website: Results of next generation sequencing of 1092 individuals from a variety of racial ethnic groups of similar diversity as HapMap phase 3. I use this datasets as a reference panel for imputation in GWAS study. My understandingHuman is diploid, 23 pairs of chromosomes, if both alleles of a human are the same, he is homozygous at that locus, otherwise it is heterozygous due to the existence of SNP. Assume on one loci of a chromosome, the base is T, normally if there is no polymorphism, it should also be T on the alleles because one from mother, one from father, if there is no SNP in the population, they should be same, but when there exists SNP, the genotype might be TC,TA,TG,CC,AA,GG. Quality Control Sample LevelFrom my own experience, it is very important to control the sample level, otherwise after you completing the whole GWAS, you will notice from the manhattan plot, some dots(variants) will suddenly jump to very abnormal level.1) IBD(Identity by Descent):]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[How to do Annotation]]></title>
    <url>%2F2018%2F10%2F30%2FHow-to-do-Annotation%2F</url>
    <content type="text"><![CDATA[Use ANNOVAR to annotate the SNPs. Prepare the ANNOVAR input file: vcf format or .avinput formatNotice: In vcf file, the reference allele column and alternative allele column is actually the major allele and minor allele, but major allele are not always the reference allele. So use R package to figure out the correct reference allele in hg19 at first. Example: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#Do the annotationsource(&quot;https://bioconductor.org/biocLite.R&quot;)#biocLite(&quot;GenomicRanges&quot;)biocLite(&quot;BSgenome&quot;)biocLite(&quot;BSgenome.Hsapiens.UCSC.hg19&quot;)library(BSgenome.Hsapiens.UCSC.hg19)#BSgenome.Hsapiens.UCSC.hg19#overviewgenome &lt;- BSgenome.Hsapiens.UCSC.hg19seqlengths(genome)seqnames(genome)Hsapiens$chr2#read in significant snp information file#library(pegas)#vcf=read.vcf(&quot;sigsnpsvcf.vcf&quot;)snpinfo=read.table(&quot;sigsnps.bim&quot;,header = FALSE,sep=&quot;\t&quot;)colnames(snpinfo)=c(&quot;CHR&quot;,&quot;SNP&quot;,&quot;Unknown&quot;,&quot;BP&quot;,&quot;Minor&quot;,&quot;Major&quot;)#to get the reference allelechrs = seqnames(Hsapiens)[1:21]sigsnpref=c()k=1for (i in 1:21) &#123; for (j in k:(k+nrow(snpinfo[which(snpinfo$CHR==i),])-1) ) &#123; sigsnpref = c(sigsnpref,as.character(getSeq(Hsapiens, names=chrs[i], start=snpinfo[j,&quot;BP&quot;], end=snpinfo[j,&quot;BP&quot;]))) &#125; k=k+nrow(snpinfo[which(snpinfo$CHR==i),])&#125;sigsnpref.dat=data.frame(sigsnpref)snpinfo$ref=sigsnpref.dat$sigsnprefsnpinfo$Minor=as.character(snpinfo$Minor)snpinfo$Major=as.character(snpinfo$Major)snpinfo$ref=as.character(snpinfo$ref)#check unmatchsnpinfo[which((snpinfo$Major)!=(snpinfo$ref)),&quot;Minor&quot;]=snpinfo[which((snpinfo$Major)!=(snpinfo$ref)),&quot;Major&quot;]snpinfo$Major=snpinfo$refsnpinfo=snpinfo[,c(1:6)]write.table(snpinfo,&quot;sigsnps.bim&quot;,row.names = FALSE,col.names=FALSE,sep = &quot;\t&quot;,quote = FALSE)#prepare the .avinput file snpinfo$SNP=snpinfo$BPsnpinfo$Unknown=snpinfo$BPsnpinfo$Alt=snpinfo$Minorsnpinfo=snpinfo[,c(1,2,3,6,7)]colnames(snpinfo)=c(&quot;CHR&quot;,&quot;Start&quot;,&quot;End&quot;,&quot;Ref&quot;,&quot;Alt&quot;)write.table(snpinfo,&quot;sigsnps.avinput&quot;,row.names = FALSE,col.names=FALSE,sep = &quot;\t&quot;,quote = FALSE) #annotation=read.table(&quot;sigannotation5*10^-5.hg19_multianno.txt&quot;,header = TRUE,sep = &quot;\t&quot;, # c(&quot;Chr&quot;,&quot;Start&quot;,&quot;End&quot;,&quot;Ref&quot;,&quot;Alt&quot;,&quot;Func.refGene&quot;,&quot;Gene.refGene&quot;,&quot;GeneDetail.refGene&quot;,&quot;ExonicFunc.refGene&quot;,&quot;AAChange.refGene&quot;)) annotation=read.csv(&quot;sigannotation2.hg19_multianno.csv&quot;,header = TRUE) annotation$BP=annotation$Start colnames(annotation)[1]=&quot;CHR&quot; sigsnps=read.csv(&quot;unilog_4covs_ph.csv&quot;,header = TRUE) combine_annotation=merge(sigsnps,annotation,by.x=c(&quot;CHR&quot;,&quot;BP&quot;),by.y=c(&quot;CHR&quot;,&quot;BP&quot;),sort = FALSE) write.table(combine_annotation,&quot;result_table.csv&quot;,sep = &quot;,&quot;,row.names = FALSE,col.names = TRUE,quote = FALSE) use plink plink --bfile filename --recode vcf --out filename to convert to vcf format, then download the database 123456789annotate_variation.pl -buildver hg19 -downdb -webfrom annovar refGene humandb/annotate_variation.pl -buildver hg19 -downdb cytoBand humandb/annotate_variation.pl -buildver hg19 -downdb -webfrom annovar exac03 humandb/ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp147 humandb/ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar dbnsfp30a humandb/ Then using table_annovar.pl to annotate in one step: 123table_annovar.pl vcffile --buildver hg19 -out outfilename -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinputtable_annovar.pl .avinputfile -buildver hg19 -out outfilename -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -csvout -polish -xref gene_reffile If using annotate_variation.pl to apply only gene based annotation 1perl annotate_variation.pl --geneanno -dbtype refGene -out outfilename -build hg19 avinputfile directory_refgene]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[master thesis]]></title>
    <url>%2F2018%2F10%2F29%2Fmaster-thesis%2F</url>
    <content type="text"><![CDATA[This is not teamwork project, this is my master thesis.]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mushroom prediction]]></title>
    <url>%2F2018%2F10%2F29%2Fmushroom-prediction%2F</url>
    <content type="text"><![CDATA[It was the final project of machine learning course. The method is very normal machine learning model. But I really think it is a good practice to build up shiny app, which is a very convenient way to present the model and results for those who do not have any backgroud of html or building up a website. The attached link is the teamwork of my team. https://ritujingyisiwei.shinyapps.io/final_mushroom/]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ggplot cheat sheet]]></title>
    <url>%2F2018%2F10%2F27%2Fggplot-cheat-sheet%2F</url>
    <content type="text"><![CDATA[ggplot(data=NULL,mapping=aes(x=,y=,color=,),environment=parent.frame()) one variableggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,))+geom_area(aes(y=..density..),stat=”bin”) #with shadow ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_density(aes(y=..county..,),kernel=”gaussian”) #without shadow ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,))++geom_dotplot()#plot dots ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight-))++geom_histogram(aes(y=..density..),binwidth=5) two variablescontinuous X, continuous Yggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_jitter() #jittering is adding a small amount of random noise to data, it is often used to spread out points that would otherwisebe overplotted ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_point() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,linetype=,size=,weight=))+geom_quantile() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,lintype=,size=,))+geom_rug(sides=”bl”) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_smooth(model=lm) ggplot(data=NULL,mapping=aes(x=,y=,label=,alpha=,angle=,color=,family=,fontface=,hjust=,lineheight=,size=,vjust=))+geom_text(aes(label=cty)) continuous bivariate distributionggplot(data=NULL,mapping=aes(xmax=,xmin=,ymax=,ymin=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_bin2d() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,linetype=,size=,))+geom_density2d() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,size=,))+geom_hex() discrete X, continuous Yggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,lintype=,size=,weight=))+geom_bar(stat=”identity”) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_boxplot(lower=,middle=,upper=,x=,ymax=,ymin=,alpha=,color=,fill=,linetype=,shape=,size=,weight=) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=))+geom_dotplot(binaxis=”y”,stackdir=”center”) visualizing errorggplot(data=NULL,aes(x=,y=,ymin=,ymax=,alpha=,color=,fill=,linetype=,size=))+geom_crossbar(fatten=2) ggplot(data=NULL,aes(x=,ymin=,ymax=,alpha=,color=,linetype=,size=,width=))+geom_errorbar() ggplot(data=NULL,aes(x=,ymin=,ymax=,alpha=,color=,linetype=,size=))+geom_linerange() ggplot(data=NULL,aes(x=,y=,ymin=,ymax=,alpha=,color=,fill=,linetype=,shape=,size=))+geom_pointrange() Statssome plots visualize a transformation of the original data set, each stats creates additional variables to map aesthetics tostat_smooth(method=,formula=)method: datasets with n1000, use gamhow to define smooths in gam formulae???s(x,k=1,fx=FALSE,bs=”tp”)x represents a list of variables that are the covariates that this smooth is a function ofk is the dimension of the basis used to represent the smooth termfx is whether the term is a fixed d.f. regression splinebs is a two letter charater string indicating the penalized smoothing basis(smooth terms in GAM)thin plate regression spline: tp;duchon splines: dscubic regression splines: cr,cs,ccsplines on the sphere: sosP-splines: psRandom effects: reMarkov Random Fields: mrfGaussian process smooths: gpsoap film smooths: so, sf, sw thin plate regression splines gives the best MSE performance but slower, the knot based penalized cubic regression splines is the secondbesthttps://www.rdocumentation.org/packages/mgcv/versions/1.8-24/topics/smooth.terms Scalesscales control how a plot maps data values to the visual values of an aestheticscale_fill_manual(values=c(),limits=c(),breaks=c(),name=” “,labels=c())specify the own set of mappings from levels in the data to aesthetic values;scale_x_continuous():set the range and the breaks for xaxis, map x values to visual valuesscale_color_manual():mapped colors for values themestheme_bw():a theme with white background and black gridlinestheme():theme(axis.text.x=element_text(size= , angle= , hjust= ):when there are many x axis coordinates, a big issue is that they will be overlapped, so change the angle of x axis coordinates and horizontal justification Facetingdivide a plot into subplots based on the values of one or more discrete variablesfacet_wrap(~fl): wraps a 1d sequence of panels into 2d(wrap facets into a rectangular layout) Legendsplace legend at bottom,top,left, or rightggplot()+geom_point()+theme(legend.position)ggplot()+geom_point()+guides(color=”none”)ggplot()+geom_point()+scale_fill_discrete(name=Title,labels=c(“A”,”B”,”C”)) some other functions not related to ggplotgsub(): replaces all matches of a stringwith(): for example, with(mtcars,summary(mpg)) &gt; to calculate the summary statistics of mpg in mtcars data, return the summary statisticcut():cut into different intervalsmelt():transform the wide format to long format; when does it need to transform to long format? when there are group factorssubset():subset datas of what you are interested in]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[apply functions in R]]></title>
    <url>%2F2018%2F10%2F27%2Fapply-functions-in-R%2F</url>
    <content type="text"><![CDATA[It takes a lot of time to run for loop, apply function is different and based on C language, which saves a lot of time. apply familyapplyapply() can be applied to matrix, dataframe, arrayapply(X,MARGIN,FUN), X: dataframe,matrix,array; MARGIN: applied to rows when =1, applied to columns when =2; FUN: function that apply to the data lapplyapply a given function to every element of a list and obtain a list as result, it can be applied to dataframes, lists or vectors, the outputreturn to a listlapply(X,FUN)lapply cannot be applied to vector or matrix sapplysimilar to lapply, but return to vector, not listsapply(X,FUN,simplify=TRUE,USE.NAMES=TRUE)X can be array, matrix, dataframesimplify: whether to arrayUSE.NAMES: if X is a string, then TRUE set string as the data nameif simplify=FALSE and USE.NAMES=FALSE, then sapply is equal to lapplyif simplify=array then can create matrixesif USE.NAMES=TRUE, then can create data name vapplyit is similar to sapplyvapply(X,FUN,FUN.VALUE,USE.NAMES=TRUE)]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Topic Model in EMR project]]></title>
    <url>%2F2018%2F10%2F26%2FTopic-Model-in-EMR-project%2F</url>
    <content type="text"><![CDATA[I did this project during my summer intern in 2017 at Duke. It was the first time I got the knowledge of Electronic Medical Record and ICD-9 codes. We focused on Duke Medical Center Electronic Medical Records from 2004 to 2013 of 210,329 patients with 10,804 unique ICD9 diagnosis codes records for each patient. The algorithm I used is the supervised Latent Dirichlet Allocation(supvised topic model). The attached is the poster of this project. This browser does not support PDFs. Please download the PDF to view it: Download PDF.]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
</search>
