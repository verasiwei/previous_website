<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docker install graph-tool]]></title>
    <url>%2F2020%2F08%2F04%2Fdocker-install-graph-tool%2F</url>
    <content type="text"><![CDATA[因为windows不能运行graph-tool库，所以了解了一下docker。我一开始的理解是类似于virtual machine, 但是了解了之后其实并不是这样，virtual machine 是指可以在一种操作系统里面运行另一种操作系统，但是virtual machine运行时会占用内存和硬盘，而且它是一个完整的系统，所以相对来说会感觉“臃肿”。 Linux开发了一种技术叫linux container，它不是一个完整的操作系统，在这个容器里面所有的进程都是虚拟的，而且它是底层系统的一个进程，这样速度就会快很多而且不会占用很多资源。 Docker的概念就是基于此，docker是linux container的一种封装， docker提供了一个接口， 把应用程序都放在一个image里，运行它就会生成一个container，程序就在这里面运行。 下面就是一个例子用docker安装graph-tool, 并且运行graph-tool。 install graph-tool using Docker 1&gt;docker pull tiagopeixoto/graph-tool run jupyter notebook inside the docker image 1234# /home/user is the virtual work directory&gt;docker run -p 8888:8888 -p 6006:6006 -it -u user -w /home/user tiagopeixoto/graph-tool bash$ jupyter notebook --ip 0.0.0.0 check the container ID 1&gt;docker ps If you closed the terminal but did not stop the container, you could enter it again. 1&gt;docker exec -it [container ID] bash you can also copy the files in docker container to local directory 1&gt;docker container cp [container ID]:[work directory] [local directory]]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Self-Study-Rcpp]]></title>
    <url>%2F2020%2F03%2F16%2FSelf-Study-Rcpp%2F</url>
    <content type="text"><![CDATA[Combine C++ code with R. 123456789101112131415#include &lt;Rcpp.h&gt;RcppExport SEXP convolve3cpp(SEXP a, SEXP b)&#123; Rcpp::NumericVector xa(a); Rcpp::NumericVector xb(b); int n_xa=xa.size(), n_xb=xb.size(); int nab=n_xa+n_xb-1; Rcpp::NumericVector xab(nab); for (int i=0; i &lt; n_xa;i++) for (int j=0; j &lt; n_xb; j++) xab[i+j] += xa[i] * xb[j]; return xab;&#125;]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Censored Regression Derivation]]></title>
    <url>%2F2019%2F12%2F15%2FCensored%20Regression%20Derivation%2F</url>
    <content type="text"><![CDATA[It seems no prediction function in R for censored regression. Here is my derivation and the corresponding R function. Reference: http://people.stern.nyu.edu/wgreene/Lugano2013/Greene-Chapter-19.pdf 1234567891011121314151617181920212223242526272829pred.censor &lt;- function(model,data,side)&#123; #get beta and sigma coefficients &lt;- summary(model)$estimate[1:(length(summary(model)$estimate[,1])-1),1] names &lt;- names(coefficients) indices &lt;- unlist(lapply(names[-1], function(x) which(colnames(data)==x))) data_pred &lt;- cbind(rep(1, length(data[,1])), do.call(cbind, lapply(indices, function(x) data[,x]))) fitted &lt;- (data_pred %*% coefficients)[,1] sigma &lt;- exp(summary(model)$estimate[length(summary(model)$estimate[,1]),1]) #predict if (side==&quot;right&quot;) &#123; right &lt;- model$right truncated &lt;- fitted - sigma*(dnorm((right - fitted)/sigma)/pnorm((right - fitted)/sigma)) prob_trunc &lt;- pnorm((right - fitted)/sigma) censored &lt;- right*(1-pnorm((right - fitted)/sigma)) + prob_trunc*truncated &#125; else if (side==&quot;left&quot;) &#123; left &lt;- model$left truncated &lt;- fitted + sigma*(dnorm((left - fitted)/sigma)/(1-pnorm((left - fitted)/sigma))) prob_trunc &lt;- 1-pnorm((left - fitted)/sigma) censored &lt;- left*pnorm((left-fitted)/sigma) + prob_trunc*truncated &#125; else &#123; left &lt;- model$left right &lt;- model$right truncated &lt;- fitted + sigma*((dnorm((left - fitted)/sigma)-(dnorm((right - fitted)/sigma)))/(pnorm((right - fitted)/sigma)-pnorm((left - fitted)/sigma))) prob_trunc &lt;- pnorm((right - fitted)/sigma) - pnorm((left - fitted)/sigma) censored &lt;- left*pnorm((left-fitted)/sigma) + prob_trunc*truncated + right*(1-pnorm((right - fitted)/sigma)) &#125; return(censored)&#125;]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Boolean Logic Regression Note]]></title>
    <url>%2F2019%2F07%2F10%2FLogic-Regression%2F</url>
    <content type="text"><![CDATA[In the regression models, we always only include the main effects and some simple interaction terms, which ignore many interactions especially when all the predictors are binary. This is the most important problem in the analysis of SNP microaray data. My master thesis is a kind of study trying to optimize this problem and validate a better approach through Iterative Sure Independence Screening developed by a mathematician, Jianqing Fan.Logic regression is a regression methodology that try to find combinations through Boolean logic of binary variables that highly predict the response. Boolean algebra is a kind of algebra that the variables values are either true or false, the operations of Boolean algebra are $\bigwedge (AND)$, $\bigvee (OR)$, $^{c} (NOT)$. The operations are the following: x \bigwedge y, if x=y=1, then x \bigwedge y=1, otherwise x \bigwedge y=0x \bigvee y, if x=y=0, then x \bigvee y=0, otherwise x \bigvee y=1x^{c}, if $x=1$, x^{c}=0The logic tree: The location of each element is a knot. The knots that do not have a parent is the root and the knots that do not have children are leaves.​ LOGIC MODELPaper reading: http://kooperberg.fhcrc.org/logic/documents/logic-regression.pdf Logic regression models: $g(E[Y])=\beta_{0} + \sum_{j=1}^{t} \beta_{j} L_{j}$, where $L_{j}$ is a Boolean expression of $X_{i}$ such like $L_{j}=X_{2} \bigwedge X_{3}$. Search for best models: movingAlternating a leaf: replace a leaf with another leaf at this position, the leaf cannot be replaced with its sibling or the complement of the sibling.Changing operators: replace $\bigwedge$ can be replaced with $bigvee$, also true for the reverse.Growing and pruning: the new branch can be grou and the counter move to grouing is pruning.Splitting and deleting: any leaf can be split by creating a sibling and any leaf can be deleted. Greedy SearchAs mentioned before, the regression model is built, a score function can be defined. For example, if linear regression, use RSS and if logistic model, use binomial deviance. The greedy algorithm is used to search best logic model, of which firstly is to find the single predictor that minimizes the score function. Then its neighbors(single move) are investigated, if a new state has a better score than the original then it will be chosen simulated annealingThe state space S is a collection of individual states and the states $S_{1}$,…,$S_{n}$ are related by a neighborhood system. The set of neighbor pairs in S defines a substructure M in S*S. The elements in M are moves, $(S,S’) \in M^{k}$ means two states S,S’ are connected via a set of k moves. All tress are fit simultaneously. We need preselect the maximum number t of trees, we select one tree in the logic model and then randomly pick a move from the move set, then refit the parameters for the new model and compare the score of the previous state and the new state, if the score is better, accept the move, if not, accept the move with a probability. model selectionTwo types of randomization tests. The first is an overall test for signal in the data. Assuming we first find the best scoring model, and we want to test whether there is association between X and Y. If there is no assocition, then Y are randomly permuted that it should have the same score as the best model. Repeat this procedure and the proportion of scores better than the best model as p value. The second is to determine the optimal model size. We want to test whether the better score obtained by models of larger sizes is due to noise. Assuming the optimal model has score $\epsilon_{j}$ and size j and sssuming the best scoring model has score $\epsilon’$ and size k. For a model with p trees, there can be up to $2^{p}$ fitted classes. Randomly permute the response, If the null hypothesis is true, the model size j is optimal, but other models of size j may have better score $\epsilon’’$ than $\epsilon_{j}$, then $\epsilon’$ would be a sample from the same distribution as $\epsilon’’$. Otherwise, optimal model had a size larger than j, then the randomization would have average worse scores than $\epsilon’$]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Bayesian machine learning:A/B Testing]]></title>
    <url>%2F2019%2F06%2F23%2FBayesian-machine-learning-A-B-Testing%2F</url>
    <content type="text"><![CDATA[Entry level, very easy Bayesian BasicDifference in Frequentist(point estimate) and Bayesian(distribution):Frequentist: $\hat{\theta}=argmax_{\theta}P(X|\theta)$Bayesian: $P(\theta|X)$ The basic is p(A|B)=p(A,B)/p(B)which is equal to p(A|B)=p(B|A)p(A)/p(B)Some rules:1.for discrete distributions, $p(B)=\sum_{A}p(A,B)=\sum_{A}p(B|A)p(A)$2.for continuous distributions, $p(A|B)=\frac{p(B|A)p(A)}{\int {p(B|A)p(A)} \, {\rm d}A}$ Some related basic concepts:1.Sensitivity: p(pred=1|disease=1)=p(pred=1,disease=1)/p(disease=1)=(TP/N)/((TP+FN)/N)=TP/(TP+FN)2.Specificity: p(pred=0|disease=0)=p(pred=0,disease=0)/p(disease=0)=TN/(TN+FP)3.Precision: TP/(TP+FP)=P(pred=1,disease=1)/p(pred=1)4.Joint Probability density, assume gaussian distribution: $p(x_{1},x_{2},…,x_{N}=\prod_{i=1}^N \frac{1}{sqrt(2\pi(\sigma)^2)} exp(-1/2 \frac{(x_{i}-\mu)^2}{(\sigma)^2})$. Traditional A/B TestingExample: You have a landing page where you get people to signup, but not everyone who visits your site will signup so that the conversion rate is the proportion of people who sign up. You want to compare your page with another new page. We know that the conversion rate(page1)=1/10 vs rate(page2)=2/10 is not as good as rate(page1)=10/100 vs rate(page2)=20/100.Simple A/B Testing Recipet test statistic $t=\frac{\overline{X_{1}} - \overline{X_{2}}}{s_{pool}\sqrt{2/N}}$ where $s_{pool}=\sqrt{\frac{(s_1)^2+(s_2)^2}{2}}$.If the size of each group is different, t test statistic $t=\frac{\overline{X_{1}} - \overline{X_{2}}}{s_{pool}\sqrt{1/n_{1}+1/n_{2}}}$.If the standard deviation of each group is different, use “Welch’s t test”.1234567891011121314151617import numpy as npfrom scipy import statsN = 10a = np.random.randn(N)+2b = np.random.randn(N)var_a = a.var(ddof=1)var_b = b.var(ddof=1)s = np.sqrt((var_a+var_b)/2)t = (a.mean()-b.mean()) / (s*np.sqrt(2/N))df = 2*N-2p = 1-stats.t.cdf(t,df=df)print &quot;t:\t&quot;, t, &quot;p:\t&quot;, 2*p#the same result as built in scipy functiont2, p2 = stats.ttest_ind(a,b) chi-square test statistic $(\chi)^2=\sum_{i} \frac{(observed_{i}-expected_{i})^2}{expected_{i}}$. Example in the course on udemy1234567891011121314151617181920212223242526272829303132333435363738import numpy as npfrom scipy.stats import chi2import matplotlib.pyplot as pltclass DataGenerator: def __init__(self,p1,p2): self.p1 = p1 self.p2 = p2 def next(self): click1 = 1 if (np.random.random()&lt;self.p1) else 0 click2 = 1 if (np.random.random()&lt;self.p2) else 0 return click1, click2def get_p_value(T): det = T[0,0]*T[1,1]- T[0,1]*T[1,0] c2 = float(det)/T[0].sum()*det/T[1].sum()*T.sum()/T[:,0].sum()/T[:,1].sum() p = 1-chi2.cdf(x=c2,df=1) return p def run_experiment(p1,p2,N): data = DataGenerator(p1,p2) p_values = np.empty(N) T = np.zeros((2,2)).astype(np.float32) for i in range(N): c1, c2 = data.next() T[0,c1] += 1 T[0,c2] += 1 if i &lt; 10: p_values[i] = None else: p_values[i] = get_p_value(T) plt.plot(p_values) #plt.plot(np.ones(N)*0.05) plt.show()run_experiment(0.1,0.11,20000) practice in the course on udemy1234567891011121314151617181920212223242526272829303132from __future__ import print_function, divisionfrom builtins import rangefrom scipy.stats import chi2, chi2_contingencyimport numpy as npimport pandas as pd#%%#read data indat = pd.read_csv(&quot;D:/machine_learning_examples/ab_testing/advertisement_clicks.csv&quot;)dat.head()a = dat[dat[&quot;advertisement_id&quot;] == &quot;A&quot;]b = dat[dat[&quot;advertisement_id&quot;] == &quot;B&quot;]a = a[&quot;action&quot;]b = b[&quot;action&quot;]#%%# function to calculate pdef get_p_value(T): det = T[0,0]*T[1,1]-T[0,1]*T[1,0] c2 = float(det) / T[0].sum() * det / T[1].sum() * T.sum() / T[:,0].sum() / T[:,1].sum() p = 1-chi2.cdf(x=c2, df=1) return pA_clk = a.sum()B_clk = b.sum()A_noclick = a.size-a.sum()B_noclick = b.size-b.sum()T = np.array([[A_clk,A_noclick],[B_clk,B_noclick]])print(get_p_value(T)) Bayesian A/B TestingThe Explore/Exploit problem: If you get 3/3 from bandit 1, 0/3 from bandit 2, should you exploit bandit 1 or explore more bandits? Or as for the advertisement A and advertisement B, if A has more clicks than B in 10 clicks, should you exploit advertisement A or explore more?Epsilon-Greedy algorithmChoose a small number of epsilon between 0 and 1, and generate a random probability, if p &lt; epsilon, then explore, otherwise exploit. UCBIDefine upper and lower limit to represent where we believe true CTR(click-through rate) is. Using a Chernoff-Hoeffding bound, $\epsilon &gt; 0$, then $P(\hat{\mu} &gt; \mu + \epsilon) \leq exp(-2(\epsilon)^2 N)$ and the opposite site $P(\hat{\mu} &lt; \mu + \epsilon) \leq exp(-2(\epsilon)^2 N)$, which is equal to say $P(|\hat{\mu} - \mu| \leq \epsilon) &gt; 1-2exp(-2(\epsilon)^2 N)$. Therefore the probability of $\mu$ in an interval is larger than a function of the number of games.We choose the epsilon $\epsilon = \sqrt{\frac{2lnN}{N_j}}$, where N is the number of total games and $N_j$ is the number of games played in bandit j. Bayesian A/B Testing P(\theta | X)=\frac{P(X|\theta)P(\theta)}{P(X)}1.Conjugate Priors: If we choose beta distributions for $P(X|\theta)$ and $P(\theta)$, then we can make $P(\theta | X)$ have the same type of distribution as $P(\theta)$.Example: P(\theta | X)=Beta(a^{'},b^{'}) where $a^{‘}=a+\sum_{i=1}{N}x_{i}$ and $b^{‘}=b+N-\sum_{i=1}{N}x_{i}$. And the prior is beta(1,1), which is equal to uniform(0,1).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#%%#Bayesian A/B Testing simulationimport matplotlib.pyplot as pltimport numpy as npfrom scipy.stats import betaNUM_TRIALS = 2000#true click-through ratesBANDIT_PROBABILITIES =[0.2,0.5,0.75]class Bandit: def __init__(self,p): self.p=p self.a=1 self.b=1 def pull(self): #less than p, get 1 that if click-through rate is 0.2, random prob less than 0.2 will return 1 so 20% will return 1. return np.random.random()&lt;self.p def sample(self): return np.random.beta(self.a,self.b) def update(self,x): self.a+=x self.b+=1-xdef plot(bandits, trial): x=np.linspace(0,1,200) for b in bandits: y=beta.pdf(x,b.a,b.b) plt.plot(x,y,label=&quot;real p: %.4f&quot; % b.p) plt.title(&quot;Bandit distributions after %s trials&quot; % trial) plt.legend() plt.show()def experiment(): bandits = [Bandit(p) for p in BANDIT_PROBABILITIES] sample_points=[5,10,20,50,100,200,500,1000,1500,1999] for i in range(NUM_TRIALS): bestb=None maxsample=-1 allsamples=[] for b in bandits: sample=b.sample() allsamples.append(&quot;%.4f&quot; % sample) if sample &gt; maxsample: maxsample=sample bestb=b if i in sample_points: print(&quot;current samples: %s&quot; % (allsamples)) plot(bandits,i) x=bestb.pull() bestb.update(x)if __name__==&quot;__main__&quot;: experiment() Thompson Sampling ConvergenceAssuming the CTR $\theta$ of each bandit follows the distribution $beta((\alpha)_k, (\beta)_k)$. Everytime producing a random number from each bandit following the current distribution, and select the bandit with largest number. If it wins, then $\alpha = \alpha +1$, otherwise $\beta =\beta +1$. And interates until the CTR converges.12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport numpy as npfrom bayesian_bandit import Banditdef run_experiment(p1, p2, p3, N): bandits = [Bandit(p1),Bandit(p2),Bandit(p3)] data = np.empty(N) for i in range(N): j=np.argmax([b.sample() for b in bandits]) x=bandits[j].pull() bandits[j].update(x) data[i]=x cumulative_average_ctr=np.cumsum(data) / (np.arange(N) + 1) plt.plot(cumulative_average_ctr) plot(np.ones(N)*p1) plot(np.ones(N)*p2) plot(np.ones(N)*p3) plt.ylim((0,1)) plt.xscale(&quot;log&quot;) plt.show()run_experiment(0.2,0.25,0.3,100000)]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Parallel work in R]]></title>
    <url>%2F2019%2F06%2F18%2FParallele-work-in-R-future%2F</url>
    <content type="text"><![CDATA[Parallel processingReferences: https://github.com/berkeley-scf/parallelR-biostat-2015/blob/master/parallel.pdfcores: the different processing units on a single nodenodes: each like one computer, each with their own memoryprocess: tasks on a machinethreads: multiple paths of execution within a single processsockets: creating new R processes With shared memory, multiple processors share the same memory. With the distributed memory, many nodes and each node will run its own memory. Running on the cores of a single node using shared memory will be faster than using the same number of cores across multiple nodes. 1.futurefutureThe difficiency of R is that R has less API. But future is a good implementation of doing parallel computing in R.Future package can use a sequential strategy, which means it will run in the current R sessions. Other strategies can be used to run parallely on the current machine or on a cluster. During parallel working, the current R session does not block, and the future are being resolved in separate processes running in the background. The source of future is here: https://github.com/HenrikBengtsson/future/tree/master. The future package implements these types of work: 1.sequential: sequentially and run in the current R session2.multisession: multiple background R sessions on current machine. The package will launch a set of R sessions in the background. If all background sessions are busy serving other futures, the creation of the next multisession future is blocked until a backgoround session becomes available again. The total number of background processes is decided by the number of available cores on your machine.12availableCores()plan(multisession) 3.multicore: forking processes. Faster than multisession but unstable. For instance, when running R from within Rstudio process forking may resulting in crashed R sessions. The future package disables multicore by default. This will cause plan(multicore) to plan(sequential) and plan(multiprocess) to plan(multisession).123#default FALSEsupportsMulticore()plan(multicore) 4.multiprocess: multicore if supported otherwise multisession. Multisession will be used unless multicore evaluation is supported.12availableCores()plan(multiprocess) Global objects have to be passed exactly as they were at the time the future was created. future.applyfuture.apply is to parallelize the apply familyI will implement asynchronous futures:1234567library(future)#firstly check how many cores in your machineavailableCores()#this will actually run multisession using up to 20 parallel futures, also 20 cores.plan(multiprocess,workers=20)resuls &lt;- future.apply::future_lapply(1:10,function(x) createfunction(x)) 2.foreach and doParallelHow about another option of using foreach and doParallel packages? Inside the doParallel package, Pay attention to there are two types of parallel working, one is multicore functionality which runs tasks on a single computer, not a cluster of computers, and another is snow functionality. And pay attention to that the packages should be load within the foreach. When using multicore, 12345678910111213141516library(doParallel)library(foreach)#multicore functionalityregisterDoParallel(cores=20)getDoParWorkers()getDoParName()#%do% is to run sequentially and %dopar% is to run parallelyresult &lt;- foreach(i=1:10) %dopar% &#123;lapply(list,function)&#125;#snow functionalitymycluster &lt;- makeCluster(5)registerDoParallel(mycluster,cores=5)getDoParWorkers()result &lt;- foreach(i=1:10) %dopar% &#123;lapply(list,function)&#125;]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning-Classification]]></title>
    <url>%2F2019%2F06%2F16%2FClassification%2F</url>
    <content type="text"><![CDATA[Bagging (Random Forest)1.Bagging and BootstrapGiven a training set $X=x_{1},x_{2},…x_{n}$ with responses $Y=y_{1}, y_{2},.. y_{n}$, bagging repeatedly with K times, each time selects a subset of random samples with replacement from the training set and fits the algorithms like trees to these samples. Assuming we have trained a regression tree $f_{k}$ on $X_{k}$ and $Y_{k}$ where k=1…K. Predictions of unknown samples $X_{unknown}$ are based on the average of the predictions from all the trees $f_{k}$ on $X_{unknown}$. f'=\frac {1}{K} \sum_{k=1}^{K} f_{k}X_{unknown}Or as for classification trees, based on the majority votes of trees. The optimal number of trees K can be found through cross-validation or by out of bag error. 2.Decision TreeA decision treee is a tree where each node represents a feature, each branch represents a dicision and each leaf represents an outcome. Tree models where the outcome are discrete values are called classification trees and tress models where the outcome take continuous values are called regression trees. 3.Random ForestRandom forest selects a random subset of the features at each candidate split in the learning process. If one or a few features are very strong predictors for the response variable, these features will be selected in many of the K trees, which cause them to become correlated.I will use the package in python and house price data in Kaggle as an implemention example. 4.Implementation1234567891011121314151617181920212223242526import pandas as pdimport numpy as npfrom sklearn.ensemble import RandomForestClassifierfrom sklearn import ensemble, tree, linear_modelfrom sklearn.model_selection import train_test_split, cross_val_score# read data intrain = pd.read_csv(&quot;D:/work/kaggle/train.csv&quot;)test = pd.read_csv(&quot;D:/work/kaggle/test.csv&quot;)# skip the procedures of data manipulation and cleaningdef train_test(estimator, x_trn, x_tst, y_trn, y_tst): prediction_train = estimator.predict(x_trn) # Printing estimator print(estimator) # Printing train scores get_score(prediction_train, y_trn) prediction_test = estimator.predict(x_tst) # Printing test scores print(&quot;Test&quot;) get_score(prediction_test, y_tst)RM = ensemble.RandomForestRegressor(n_estimators=1000,random_state=0,criterion=&quot;mse&quot;,max_features=&quot;sqrt&quot;,max_depth=50).fit(x_train,y_train)train_test(RM,x_train,x_test,y_train,y_test) Boosting (Gradient Boosting)1.AlgorithmThe aim of the regression is to fit a model F(x) to predict the outcome $\hat{y}$ by minimizing the error. For example, in the situation of least-square regreesion setting, the aim of the regression is to fit a model F(x) to predict the outcome $\hat{y}$ by minimizing the mean squared error $\frac{1}{n} \sum_{i}(\hat{y_{i}}-y_{i})^2$. At each stage k, $1 \leq k \leq K$, the function at each stage is $F_{k}$. The gradient boosting improves on the model by constructing a new model to estimate the error: $F_{k+1}(x)=F_{k}(x)+h(x)$. Assuming $\hat{y}=F(x)$ and the loss function is $L(y,\hat{y})$. The error for all samples are $J=\sum_{i=1}^{n} L(y_i,\hat{y_i}$. Based on the chapter of algebra basic, the gradient of J is $\nabla J=\hat{y} -y$. Therefore the algorithm of gradient boosting is:1.initialize a model $F_{0}(x)$2.For k=1 to K:2.1compute gradient r_{ki}=-\frac {\alpha L(y_{i}, F(x_{i}))}{\alpha F(x_{i})}2.2Fit a function $h_{k}$ by using the training dataset 2.3compute $\gamma$ by solving the below equation: \gamma_{k}=argmin \sum_{i=1}^{n}L(y_{i},F_{k-1}(x_{i})+\gamma h_{k}(x_{i}))2.4Update the model F_{k}(x)=F_{k-1}(x)+\gamma_{k}h_{k}(x)2.ImplementationI will use package in Python as an example. 12345from sklearn import ensemble, tree, linear_modelfrom sklearn.ensemble import GradientBoostingRegressorGBest = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features=&apos;sqrt&apos;, min_samples_leaf=15, min_samples_split=10, loss=&apos;huber&apos;).fit(x_train, y_train)]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra Basic]]></title>
    <url>%2F2019%2F06%2F13%2FAlgebra_Basic%2F</url>
    <content type="text"><![CDATA[This is the review and cheatsheet of some basic linear algebra knowledge during my undergraduate study. 1. BasicDeterminantA scalar value that can be computed from elements of a square matrix. det(A)=|A|=\sum_{j=1}^{n}(-1)^{i+j}a_{i,j}M_{i,j}where $a_{ij}$ is the element of A and $M_{ij}$ is minor. Matrix InverseA square matrix M has an inverse that M is invertible if the determinant $|M| \neq 0$. A^{-1}A=IThe inverse properties are1.(A^{-1})^{-1}=A2.(AB)^{-1}=B^{-1}A^{-1}3.(A^{-1})^{T}=(A^{T})^{-1} Orthogonal MatricesA square matrix $A \in R^{nn}$ is orthogonal: A^{T}A=I=AA^{T}Matrix multiplication\(A \in R^{mn}\) and \(B \in R^{np}\) C=AB=\sum_{k=1}^{n}A_{ik}B_{kj}Matrix transpose (A^{T})^T=A(AB)^{T}=B^{T}A^{T}(A+B)^{T}=A^{T}+B^{T}TraceThe trace of a square matrix $A \in R^{nn}$ is denoted as tr(A), which is the sum of diagonal elements in the matrix:$tr(A)=\sum_{i=1}^{n}A_{ii}$. Properties are below:1.trA=trA^{T}2.tr(A+B)=trA+trB3.tr(tA)=t tr(A)4.tr(AB)=tr(BA)5.tr(ABC)=tr(BCA)=tr(CAB) RankThe column rank of a matrix $A \in R^{mn}$ is the size of the largest subset of columns of A that constitute a linearly independent set. The same definition as for row rank. For any matrix, the column rank is equal to the row rank. Both are denoted as rank(A). Properties are below:1.rank(A) \leq min(m,n)2.rank(A)=rank(A^{n}3.rank(AB) \leq min(rank(A), rank(B))4.rank(A+B) \leq rank(A)+rank(B) Eigenvalue and EigenvectorsGiven a square matrix A, \(\lambda\) is an eigenvalue and x is the corresponding eigenvector if Ax=\lambda x, x \neq 0which equals to (\lambda I-A)x=0The properties are1.tr(A)=\sum_{i=1}^{n} \lambda_{i}2.|A|=\prod{i=1}^n \lambda_{i}3.The rank of A is equal to the number of non-zero eigenvalues of A4.If A is non-singular then $1/\lambda_{i}$ is an eigenvalue of $A^{-1}$ with associated eigenvector.5.The eigenvalues of a diagonal matrix are just the diagonal entries. The gradient$f: R^{mn} \to R$ is a function that input a matrix A and returns a value. The gradient of f is the matrix of partial derivatives. \left[ \begin{matrix} \alpha f(A)/(\alpha A_{11}) & \alpha f(A)/(\alpha A_{12}) & ... \alpha f(A)/(\alpha A_{1n}) \\\\ \alpha f(A)/(\alpha A_{21}) & \alpha f(A)/(\alpha A_{22}) & ... \alpha f(A)/(\alpha A_{2n}) \\\\ ...... \\\\ \alpha f(A)/(\alpha A_{m1}) & \alpha f(A)/(\alpha A_{m2}) & ... \alpha f(A)/(\alpha A_{mn}) \\\\ \end{matrix} \right]2.Other definitions and calculationsLaplace Matrix (simple graph)Given a simple graph G with n vertices, its Laplace Matrix \(L_{nn}\) is defined as: L=D-A, where D is the degree matrix and A is the adjacency matrix. D is a diagonal matrix which includes the information about the degree of each vertex. A is the adjacency matrix which only includes 1 and 0 since G is a simple graph and the diagonal are all 0.symmetrix normalized laplacian L^{sym}=D^{-1/2}LD^{-1/2}=I-D^{-1/2}AD^{-1/2}Singular value decompositionAssume $A \in R^{mn}$ and all elements in M belongs to real or plural values. There exists a decomposition that A=U \sum V^{T}where U and V are orthogonal that $U^{T}U=I_{mm}$ and $V^{T}V=I_{nn}$. A^{T}A=V (\sum)^{T} \sum V^{T}AA^{T}=U \sum (\sum)^{T} U^{T}Eigen decomposition$A \in R^{nn}$ is a square matrix with n linear independent eigenvectors $q_{i}$(i=1…n). A can be factorized as A=Q\Lambda Q^{-1}where Q is the square n by n matrix with ith column is the eigenvector of A, and $\Lambda$ is the diagonal matrix with diagonal elements are the corresponding eigenvalues.]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mixed Effect Model]]></title>
    <url>%2F2019%2F05%2F01%2FMixed-Effect-Model%2F</url>
    <content type="text"><![CDATA[Assuming there is a continuous outcome measure y, and the interest is to examine the relationship between y and a continuous variable called $x_{1}$ that the model is something like this $y~\beta_{0}+\beta_{1}x_{1}+\epsilon$. And assuming we also have age and sex fixed effects in the model, so the model is like this $y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}age+\beta_{3}sex+\epsilon$. But the situation is that for each subject, we took multiple measures at different timepoints that each subject have multiple responses at different timepoints, so multiple responses from the same subject are not independent. We need to add random effect in the regression model to deal with this situation. Each subject might have different “baseline” y value that subject 1 might have a mean y value1 and subject 2 might have a mean y value2 across three timepoints. Therefore we need to include subjects as one random effect. $y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}age+\beta_{3}sex+\gamma_{1}subject+\epsilon$]]></content>
      <categories>
        <category>Statistics &amp; Algorithms</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Self-Study-SQL-Notebook]]></title>
    <url>%2F2019%2F03%2F27%2FSelf-Study-SQL-Notebook%2F</url>
    <content type="text"><![CDATA[SQL is a very simple modular query language. I have a bad memory that sometimes I really need a cheatsheet ~_~ PostgreSQL FundamentalSELECT1) select all columnsSELECT * FROM table; 2) select some columnsSELECT column1,column2,… FROM table; 3) select distinct statement in a tableSELECT DISTINCT column1,column2,…FROM table; 4) select where statementSELECT column1,column2,… FROM tableWHERE column1 &lt;&gt;= m AND column2 &gt;= n ; COUNT1) count does not consider NULLSELECT COUNT(DISTINCT column) FROM table; LIMIT1) specify how many rows to selectSELECT * FROM tableLIMIT 5; ORDER BYSELECT column_1,column2 FROM table_nameORDER BY column_1 ASC,column_2 DESC; BETWEENSELECT column_1,column2 FROM table_nameWHERE column_1 BETWEEN m AND n; INSELECT column_1, column2 FROM table_nameWHERE column_1 IN (1,2)ORDER BY column_2 DESC; LIKE1)SELECT column_1,column_2 FROM table_nameWHERE column_1 LIKE ‘xxx%’; 2)sensitive to case charactersSELECT column_1,column_2 FROM table_nameWHERE column_1 ILIKE ‘%xxx’; 3)SELECT column_1,column_2 FROM table_nameWHERE column_1 NOT LIKE ‘%xxx%’; GROUP BY StatementsMIN, MAX, AVG, SUM1)SELECT ROUND(AVG(column_1),m) FROM table_name; 2)SELECT ROUND(MIN(column_1),m) FROM table_name; 3)SELECT ROUND(MAX(column_1),m) FROM table_name; 4)SELECT ROUND(SUM(column_1),m) FROM table_name; GROUP BY1)SELECT column_1, SUM(column_2) FROM table_nameGROUP BY column_1ORDER BY SUM(column_2) DESC; 2)SELECT column_1, COUNT(*) FROM table_nameGROUP BY column_1; 3)SELECT column_1, COUNT(column_1), SUM(column_1)FROM table_nameGROUP BY column_1; HAVING1) The condition applies to the group rows created by the GROUP BY, but WHERE applies before GROUP BYSELECT column_1, aggregate_function(column_2)FROM table_nameWHERE column_1 IN (“”,””,””)GROUP BY column_1HAVING condition; JOINSAS1)SELECT column_1, SUM(column_2) AS nameFROM table_nameGROUP BY column_1; Inner Join1)SELECET column_1, column_2, column_3, column_4FROM table1_nameINNER JOIN table2_name ON table1_name.column_1 = table2_name.column_1; 2)SELECT column_1, column_2 AS name_1FROM table1_nameJOIN table2_name AS name_2 ON name_2.column_1=table1_name.column_1; FULL OUTER JOIN1) The set of all records in table 1 and table 2, the missingness will contain NULLSELECT * FROM table1_nameFULL OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2 LEFT OUTER JOIN1) A complete set of records from table 1 and the matching records in table 2, the missingness of right table will be NULLSELECT * FROM table1_nameLEFT OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2 2) the records only in table 1 but not in table 2SELECT * FROM table1_nameLEFT OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2WHERE table1_name.id IS null FULL OUTER JOIN1) the records unique to table 1 and table 2SELECT * FROM table1_nameFULL OUTER JOIN table2_nameON table1_name.column_1 = table2_name.column_2WHERE table1.id IS null OR table2.id IS null UNION1) all records in table 1 and table 2 even some are sameSELECT column_1,column_2FROM table1_nameUNION ALLSELECT column_1,column_2FROM table2_name; 2) removes all duplicate rowsSELECT column_1,column_2FROM table1_nameUNIONSELECT column_1,column_2FROM table2_name; Timestamps and Extract1) Extract daySELECT extract(day from column_1) FROM table_name;2) Extract monthSELECT extract(month from column_1) FROM table_name; Mathematical Functions1) additionSELECT column_1+column_2 AS newcolumnFROM table_name; 2) multiplySELECT column_1*column_2 AS newcolumnFROM table_name; 3) divitionSELECT column_1/column_2 AS newcolumnFROM table_name; String Functions1)SELECT column_1 || ‘ ‘ || column_2 FROM table_name; 2)SELECT lower(column_1) FROM table_name; Subquery1)SELECT column_1, column_2, column_3 FROM table_nameWHEREcolumn_3 &gt; (SELECT AVG(column_3) FROM table_name); 2)SELECT table1_name.column_1FROM table1_nameINNER JOIN table2_name ON table1_name.column_1 = table2_name.column_1WHEREcolumn_2 BETWEEN ‘’ AND ‘’; Self Join1) SubquerySELECT column_1 FROM table1_nameWHERE column_2 IN(SELECT column_2 FROM table2_name)WHERE column_1 = “ “) 2) Self joinSELECT table1_name.column_1FROM table1_name AS newname1, table1_name AS newname2WHEREnewname1.column_2 = newname2.column_2AND newname2.column_1=” “; Creating Databases and Tables1) Data typescharacter:char(n)integers: int(n)real: float(n)numeric: numeric(n) 2) Create tableCREATE TABLE table_name ( column_name data_type column_constraint, table_constraint)INHERITS existing_table_name; 3) INSERTINSERT INTO table(column1,column2)VALUES (value1,value2),VALUES (value1,value2); Insert from another tableINSERT INTO tableSELECT column1,column2,…FROM another_tableWHERE condition; 4) UPDATEUPDATE tableSET column1 = value1, column2 = value2,…WHERE conditionRETURNING column1, column2,…; 5) DELETEDELETE FROM tableWHERE condition 6) Alter TableALTER TABLE table_name DROP COLUMN column_1;ALTER TABLE table_name ADD COLUMN column_1 boolean;ALTER TABLE table_name RENAME COLUMN column1 TO new_column1_name; 7) Drop TableDROP TABLE IF EXISTS table_name RESTRICT; 8) Check ConstraintCREATE TABLE table_name( column_1 serial PRIMARY KEY, column_2 VARCHAR(50), column_3 DATE CHECK(column_3 &gt; ‘1900-01-01’), column_4 DATE CHECK(column_4 &gt; column_3), column_5 integer CHECK(column_5&gt;0));]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Self-Study AWS Notebook]]></title>
    <url>%2F2019%2F03%2F12%2FSelf-Study-AWS-Notebook%2F</url>
    <content type="text"><![CDATA[Updating……They are all modular things, a cheatsheet to save time when I cannot remember something StorageAWS S3Block storage: stores data organized as an array of unrelated blocksFile storage: stores data in a hierarchy of files and foldersObject storage: stores data as objects which consist of data, metadata and identifier Buckets: Create a bucket and store data/Pay only what you use/Region basedObjects: Objects stored in buckets(image, word…) Object metadta=Name | ValueObject storage classes: S3 standard; S3 intelligent-tiering; S3 standard-IA; S3 One Zone-IA; S3 Glacier 1) insert your AWS access key ID and secret access key1234D:\&gt; aws configureD:\&gt; aws s3api list-bucketsD:\&gt; aws s3 ls s3://D:\&gt; aws s3api head-object --bucket bucketname --key objectname 2) S3 access permission: how to add permission of another account? https://docs.aws.amazon.com/quicksight/latest/user/using-s3-files-in-another-aws-account.html12345678910111213#Object#abort multipart uploads3:AbortMultipartUpload#delete objects3:DeleteObject#get object, head object, select object contents3:GetObject#Bucket#get public access blocks3:GetAccountPublicAccessBlock#put public accessblock, delete public access blocks3:PutAccountPublicAccessBlock 3) Create new bucket:1aws s3api create-bucket --bucket bucket_name 4) Create new objects(prefix) within bucket:1aws s3api put-object --bucket bucket_name --key foldername/ 5) Check bucket size1aws s3 ls --summarize --human-readable --recursive s3://bucket_name/ ComputeAWS EC2EC2 is the AWS to create and run virtual machines in the cloud, the VM is called “instances”. To use an SSH client to connect to an linux instance1ssh -i /path/MyKeyPair.pem ec2-user@IP_address Move data to and from S3 and EC2 instance12345ec2-user@IPaddress $ wget https://bucket.s3.amazonaws.com/path-to-fileorec2-user@IPaddress $ aws s3 cp s3://bucket/path-to-file filenameor download an entire S3 bucket to a directory on the instanceec2-user@IPaddress $ aws s3 sync s3://bucket directory_on_instance AnalyticsAmazon QuickSight1) creating a data set using S3 files2) creating a data set using Amazon Athena Data: https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Rent Apartment in Nashville]]></title>
    <url>%2F2019%2F03%2F06%2FRent%20Apartment%20in%20Nashville%2F</url>
    <content type="text"><![CDATA[Rent Apartment in Nashvilleprerequisite: SAFE! SAFE! SAFE! Gated community! Gated community! Gated community! better: In unit laundry; Top floors Next to Vanderbilt: Apartment 1) West end village: 2) Americana Apartments Condo(Personal Landlord) 1) Continental Condo]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cheatsheet Shell Scripts]]></title>
    <url>%2F2019%2F01%2F04%2FCheatsheet-Shell-Scripts%2F</url>
    <content type="text"><![CDATA[Something useful for shell scripts/gawk scripts/git commands/aws commandscreate public ssh key12$ ssh-keygen -t rsa -C &quot;siwei&quot;$ cat ~/.ssh/id_rsa.pub connect to the remote server and modify the files on the server remotely123#download rmate firstly$ ssh -R 52698:127.0.0.1:52698 username@server $ rmate -p 52698 filename change group of files12#change all files under this folder$ chgrp -R group file change permission12345678#have a look of the permission of a folder/file$ getfacl &quot;file&quot;#change the permission of a user read/write/execute$ setfacl -m u:userID:rwx &quot;file&quot;#change the permission of a group read/write/execute $ setfacl -m g:groupID:rwx &quot;file&quot; deal with gzipped files12345678#have a look of compressed files$ zcat file | head#uncompress files$ gunzip -c file &gt; /directory/file#compress files$ gzip -c file &gt; /directory/file loop123$for i in `seq 1 22`;dogawk -v chr=$&#123;i&#125; &apos;$5==&quot;SNP&quot; &#123;print chr&quot; &quot;$2&#125;&apos; &gt;&gt; filedone ifelse1234567$ if [&quot;answer&quot; == &quot;local&quot;]; thenfor i in `seq 1 22`;doecho &quot;hello world&quot; &gt;&gt; file1doneelseecho &quot;hello world&quot; &gt;&gt; file2fi git commands1234567#one local repository to multiple remote repositories$ git remote add both repository1$ git remote remote set-url --add --push both repository1$ git remote set-url --add --push both repository2$ git add .$ git commit -m first_commit$ git push both aws commandsReference: from AWS tutorialAmazon S3 is a repository for internet data. It is designed to store and retrieve any amount of data from Amazon EC2. Amazon EC2 uses Amazon S3 for storing Amazon Machine Images(AMIs), users use AMI for launching EC2 instances. 1234#to get started$ aws configure#list the contents in S3$ aws s3api list-buckets]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GWAS Pipeline]]></title>
    <url>%2F2018%2F12%2F31%2FGWAS-Pipeline%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[灯影牛肉]]></title>
    <url>%2F2018%2F12%2F16%2F%E7%81%AF%E5%BD%B1%E7%89%9B%E8%82%89%2F</url>
    <content type="text"><![CDATA[灯影牛肉1.买bottom round或者top round部位,不要买牛腱,因为太筋道不好撕，附上牛身上各部分的图片2.用料酒泡,再用水煮一下去除血水3.instant pot里放入肉,放大料,八角,桂皮,酱油,一包卤料,加水没过牛肉4.按meatstew按钮,时间调成一小时5.完事了卤牛肉6.然后晾干牛肉,撕成丝7.锅里放大把的油,五香粉,辣椒粉,大把的糖,倒入牛肉丝翻炒8.all done!至于我为什么会做这么复杂又耗时间的一道菜,原因是本来我想做卤牛肉的,没成想没买到牛腱,做出来的卤牛肉切不成片,口感又老,所以改成了这道菜]]></content>
      <categories>
        <category>cook menu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[deign_analysis_interpretation_GWAS]]></title>
    <url>%2F2018%2F11%2F19%2Fdeign-analysis-interpretation-GWAS%2F</url>
    <content type="text"><![CDATA[Notes for the book: Design, Analysis, and Interpretation of Genome-Wide Association Scans Introduction:Charles Darwin’s workLed most directly to the field of genetics, which is the theory of evolution: 1) More individuals are produced each generation that survive. 2) Phenotypic variation exists among individuals and the variation is heritable. 3) Those individuals with heriatble traits better suited to the environment will survive. 4) When reproductive isolation occurs new species will form [Wiki] Fundamental to the emergence of modern genetics was the discovery by Gregor Mendel:1) Law of Segregation: When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. 2) Law of Independent Assortment: Each pair of alleles segregates independently of the other pairs of alleles during gamete formation. 3) Law of Dominance: If the two alleles of an inherited pair differ, then one determines the organism’s appearance and is called the dominant allele. [Wiki] Organization of Chromosomes:22 autosomes and 2 sex chromosomes. The Centromere (which I circled in the example picture[Wiki]) divides the chromosome into two arms, the shorter arm is the p arm and the longer is the q arm. Example: 1q22 is the 2th band of the 2nd region of the long arm of chromosome 1. Organization of DNA:Each chromosome is made up of two strands of DNA, each strand has a deoxyribose backbone, each sugar having a 3’ carbon linked by a phosphate group to the 5’ carbon. The strand issue is the problem when compare SNP data from different genotyping platforms. If one platform uses probes for one strand and another platform uses the reverse probse, it will be read differently. So define “plus” or “forward” strand for a given chromosome as the strand of DNA for which the 5’ end is the nearest to the centromere, another definition is moving from the 5’ to the 3’ end moves in increasing base position order. The “minor” or “reverse” strand is the complementary strand to plus strand. Example in the picture: Types of Genetic Variation1) An SNV consists of a single base variation at a specific position on a given strand of DNA with the change measured relative to some existing consensus sequence. If an SNV presents at least 1%, it will be denoted as SNP.2) Insertions/Deletions3) Larger Structured Variants: including deletions, insertions, and duplications of segments of DNA that are 1kb or greater in extent are termed copy number variants or CNVs. Other types of structural variants include inversions and translocations.4) Exonic Variation: Mendelian traits are most often due to protein changes in specific genes.5) Non-exonic SNPs and Disease6) SNP Haplotypes: a combination of from several to many SNPs on the same chromosome segment of DNA. Basic Genomics:dbSNP: SNPs and other variants are reported.UCSC Genome Browser: tools and datasets compiling sequence information for many genome. The blat tool maps sequences to genome location and strand is also available?1000 Genomes Project website: Results of next generation sequencing of 1092 individuals from a variety of racial ethnic groups of similar diversity as HapMap phase 3. I use this datasets as a reference panel for imputation in GWAS study. My understandingHuman is diploid, 23 pairs of chromosomes, if both alleles of a human are the same, he is homozygous at that locus, otherwise it is heterozygous due to the existence of SNP. Assume on one loci of a chromosome, the base is T, normally if there is no polymorphism, it should also be T on the alleles because one from mother, one from father, if there is no SNP in the population, they should be same, but when there exists SNP, the genotype might be TC,TA,TG,CC,AA,GG. Quality Control Sample LevelFrom my own experience, it is very important to control the sample level, otherwise after you completing the whole GWAS, you will notice from the manhattan plot, some dots(variants) will suddenly jump to very abnormal level.1) IBD(Identity by Descent):]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[How to do Annotation]]></title>
    <url>%2F2018%2F10%2F30%2FHow-to-do-Annotation%2F</url>
    <content type="text"><![CDATA[Use ANNOVAR to annotate the SNPs. Prepare the ANNOVAR input file: vcf format or .avinput formatNotice: In vcf file, the reference allele column and alternative allele column is actually the major allele and minor allele, but major allele are not always the reference allele. So use R package to figure out the correct reference allele in hg19 at first. Example: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#Do the annotationsource(&quot;https://bioconductor.org/biocLite.R&quot;)#biocLite(&quot;GenomicRanges&quot;)biocLite(&quot;BSgenome&quot;)biocLite(&quot;BSgenome.Hsapiens.UCSC.hg19&quot;)library(BSgenome.Hsapiens.UCSC.hg19)#BSgenome.Hsapiens.UCSC.hg19#overviewgenome &lt;- BSgenome.Hsapiens.UCSC.hg19seqlengths(genome)seqnames(genome)Hsapiens$chr2#read in significant snp information file#library(pegas)#vcf=read.vcf(&quot;sigsnpsvcf.vcf&quot;)snpinfo=read.table(&quot;sigsnps.bim&quot;,header = FALSE,sep=&quot;\t&quot;)colnames(snpinfo)=c(&quot;CHR&quot;,&quot;SNP&quot;,&quot;Unknown&quot;,&quot;BP&quot;,&quot;Minor&quot;,&quot;Major&quot;)#to get the reference allelechrs = seqnames(Hsapiens)[1:21]sigsnpref=c()k=1for (i in 1:21) &#123; for (j in k:(k+nrow(snpinfo[which(snpinfo$CHR==i),])-1) ) &#123; sigsnpref = c(sigsnpref,as.character(getSeq(Hsapiens, names=chrs[i], start=snpinfo[j,&quot;BP&quot;], end=snpinfo[j,&quot;BP&quot;]))) &#125; k=k+nrow(snpinfo[which(snpinfo$CHR==i),])&#125;sigsnpref.dat=data.frame(sigsnpref)snpinfo$ref=sigsnpref.dat$sigsnprefsnpinfo$Minor=as.character(snpinfo$Minor)snpinfo$Major=as.character(snpinfo$Major)snpinfo$ref=as.character(snpinfo$ref)#check unmatchsnpinfo[which((snpinfo$Major)!=(snpinfo$ref)),&quot;Minor&quot;]=snpinfo[which((snpinfo$Major)!=(snpinfo$ref)),&quot;Major&quot;]snpinfo$Major=snpinfo$refsnpinfo=snpinfo[,c(1:6)]write.table(snpinfo,&quot;sigsnps.bim&quot;,row.names = FALSE,col.names=FALSE,sep = &quot;\t&quot;,quote = FALSE)#prepare the .avinput file snpinfo$SNP=snpinfo$BPsnpinfo$Unknown=snpinfo$BPsnpinfo$Alt=snpinfo$Minorsnpinfo=snpinfo[,c(1,2,3,6,7)]colnames(snpinfo)=c(&quot;CHR&quot;,&quot;Start&quot;,&quot;End&quot;,&quot;Ref&quot;,&quot;Alt&quot;)write.table(snpinfo,&quot;sigsnps.avinput&quot;,row.names = FALSE,col.names=FALSE,sep = &quot;\t&quot;,quote = FALSE) #annotation=read.table(&quot;sigannotation5*10^-5.hg19_multianno.txt&quot;,header = TRUE,sep = &quot;\t&quot;, # c(&quot;Chr&quot;,&quot;Start&quot;,&quot;End&quot;,&quot;Ref&quot;,&quot;Alt&quot;,&quot;Func.refGene&quot;,&quot;Gene.refGene&quot;,&quot;GeneDetail.refGene&quot;,&quot;ExonicFunc.refGene&quot;,&quot;AAChange.refGene&quot;)) annotation=read.csv(&quot;sigannotation2.hg19_multianno.csv&quot;,header = TRUE) annotation$BP=annotation$Start colnames(annotation)[1]=&quot;CHR&quot; sigsnps=read.csv(&quot;unilog_4covs_ph.csv&quot;,header = TRUE) combine_annotation=merge(sigsnps,annotation,by.x=c(&quot;CHR&quot;,&quot;BP&quot;),by.y=c(&quot;CHR&quot;,&quot;BP&quot;),sort = FALSE) write.table(combine_annotation,&quot;result_table.csv&quot;,sep = &quot;,&quot;,row.names = FALSE,col.names = TRUE,quote = FALSE) use plink plink --bfile filename --recode vcf --out filename to convert to vcf format, then download the database 123456789annotate_variation.pl -buildver hg19 -downdb -webfrom annovar refGene humandb/annotate_variation.pl -buildver hg19 -downdb cytoBand humandb/annotate_variation.pl -buildver hg19 -downdb -webfrom annovar exac03 humandb/ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp147 humandb/ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar dbnsfp30a humandb/ Then using table_annovar.pl to annotate in one step: 123table_annovar.pl vcffile --buildver hg19 -out outfilename -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinputtable_annovar.pl .avinputfile -buildver hg19 -out outfilename -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -csvout -polish -xref gene_reffile If using annotate_variation.pl to apply only gene based annotation 1perl annotate_variation.pl --geneanno -dbtype refGene -out outfilename -build hg19 avinputfile directory_refgene]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[master thesis]]></title>
    <url>%2F2018%2F10%2F29%2Fmaster-thesis%2F</url>
    <content type="text"><![CDATA[This is not teamwork project, this is my master thesis.]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mushroom prediction]]></title>
    <url>%2F2018%2F10%2F29%2Fmushroom-prediction%2F</url>
    <content type="text"><![CDATA[It was the final project of machine learning course. The method is very normal machine learning model. But I really think it is a good practice to build up shiny app, which is a very convenient way to present the model and results for those who do not have any backgroud of html or building up a website. The attached link is the teamwork of my team. https://ritujingyisiwei.shinyapps.io/final_mushroom/]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ggplot cheat sheet]]></title>
    <url>%2F2018%2F10%2F27%2Fggplot-cheat-sheet%2F</url>
    <content type="text"><![CDATA[ggplot(data=NULL,mapping=aes(x=,y=,color=,),environment=parent.frame()) one variableggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,))+geom_area(aes(y=..density..),stat=”bin”) #with shadow ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_density(aes(y=..county..,),kernel=”gaussian”) #without shadow ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,))++geom_dotplot()#plot dots ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight-))++geom_histogram(aes(y=..density..),binwidth=5) two variablescontinuous X, continuous Yggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_jitter() #jittering is adding a small amount of random noise to data, it is often used to spread out points that would otherwisebe overplotted ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_point() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,linetype=,size=,weight=))+geom_quantile() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,lintype=,size=,))+geom_rug(sides=”bl”) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_smooth(model=lm) ggplot(data=NULL,mapping=aes(x=,y=,label=,alpha=,angle=,color=,family=,fontface=,hjust=,lineheight=,size=,vjust=))+geom_text(aes(label=cty)) continuous bivariate distributionggplot(data=NULL,mapping=aes(xmax=,xmin=,ymax=,ymin=,alpha=,color=,fill=,linetype=,size=,weight=))+geom_bin2d() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,linetype=,size=,))+geom_density2d() ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,size=,))+geom_hex() discrete X, continuous Yggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,lintype=,size=,weight=))+geom_bar(stat=”identity”) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=,shape=,size=,))+geom_boxplot(lower=,middle=,upper=,x=,ymax=,ymin=,alpha=,color=,fill=,linetype=,shape=,size=,weight=) ggplot(data=NULL,mapping=aes(x=,y=,alpha=,color=,fill=))+geom_dotplot(binaxis=”y”,stackdir=”center”) visualizing errorggplot(data=NULL,aes(x=,y=,ymin=,ymax=,alpha=,color=,fill=,linetype=,size=))+geom_crossbar(fatten=2) ggplot(data=NULL,aes(x=,ymin=,ymax=,alpha=,color=,linetype=,size=,width=))+geom_errorbar() ggplot(data=NULL,aes(x=,ymin=,ymax=,alpha=,color=,linetype=,size=))+geom_linerange() ggplot(data=NULL,aes(x=,y=,ymin=,ymax=,alpha=,color=,fill=,linetype=,shape=,size=))+geom_pointrange() Statssome plots visualize a transformation of the original data set, each stats creates additional variables to map aesthetics tostat_smooth(method=,formula=)method: datasets with n1000, use gamhow to define smooths in gam formulae???s(x,k=1,fx=FALSE,bs=”tp”)x represents a list of variables that are the covariates that this smooth is a function ofk is the dimension of the basis used to represent the smooth termfx is whether the term is a fixed d.f. regression splinebs is a two letter charater string indicating the penalized smoothing basis(smooth terms in GAM)thin plate regression spline: tp;duchon splines: dscubic regression splines: cr,cs,ccsplines on the sphere: sosP-splines: psRandom effects: reMarkov Random Fields: mrfGaussian process smooths: gpsoap film smooths: so, sf, sw thin plate regression splines gives the best MSE performance but slower, the knot based penalized cubic regression splines is the secondbesthttps://www.rdocumentation.org/packages/mgcv/versions/1.8-24/topics/smooth.terms Scalesscales control how a plot maps data values to the visual values of an aestheticscale_fill_manual(values=c(),limits=c(),breaks=c(),name=” “,labels=c())specify the own set of mappings from levels in the data to aesthetic values;scale_x_continuous():set the range and the breaks for xaxis, map x values to visual valuesscale_color_manual():mapped colors for values themestheme_bw():a theme with white background and black gridlinestheme():theme(axis.text.x=element_text(size= , angle= , hjust= ):when there are many x axis coordinates, a big issue is that they will be overlapped, so change the angle of x axis coordinates and horizontal justification Facetingdivide a plot into subplots based on the values of one or more discrete variablesfacet_wrap(~fl): wraps a 1d sequence of panels into 2d(wrap facets into a rectangular layout) Legendsplace legend at bottom,top,left, or rightggplot()+geom_point()+theme(legend.position)ggplot()+geom_point()+guides(color=”none”)ggplot()+geom_point()+scale_fill_discrete(name=Title,labels=c(“A”,”B”,”C”)) some other functions not related to ggplotgsub(): replaces all matches of a stringwith(): for example, with(mtcars,summary(mpg)) &gt; to calculate the summary statistics of mpg in mtcars data, return the summary statisticcut():cut into different intervalsmelt():transform the wide format to long format; when does it need to transform to long format? when there are group factorssubset():subset datas of what you are interested in]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[apply functions in R]]></title>
    <url>%2F2018%2F10%2F27%2Fapply-functions-in-R%2F</url>
    <content type="text"><![CDATA[It takes a lot of time to run for loop, apply function is different and based on C language, which saves a lot of time. apply familyapplyapply() can be applied to matrix, dataframe, arrayapply(X,MARGIN,FUN), X: dataframe,matrix,array; MARGIN: applied to rows when =1, applied to columns when =2; FUN: function that apply to the data lapplyapply a given function to every element of a list and obtain a list as result, it can be applied to dataframes, lists or vectors, the outputreturn to a listlapply(X,FUN)lapply cannot be applied to vector or matrix sapplysimilar to lapply, but return to vector, not listsapply(X,FUN,simplify=TRUE,USE.NAMES=TRUE)X can be array, matrix, dataframesimplify: whether to arrayUSE.NAMES: if X is a string, then TRUE set string as the data nameif simplify=FALSE and USE.NAMES=FALSE, then sapply is equal to lapplyif simplify=array then can create matrixesif USE.NAMES=TRUE, then can create data name vapplyit is similar to sapplyvapply(X,FUN,FUN.VALUE,USE.NAMES=TRUE)]]></content>
      <categories>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Topic Model in EMR project]]></title>
    <url>%2F2018%2F10%2F26%2FTopic-Model-in-EMR-project%2F</url>
    <content type="text"><![CDATA[I did this project during my summer intern in 2017 at Duke. It was the first time I got the knowledge of Electronic Medical Record and ICD-9 codes. We focused on Duke Medical Center Electronic Medical Records from 2004 to 2013 of 210,329 patients with 10,804 unique ICD9 diagnosis codes records for each patient. The algorithm I used is the supervised Latent Dirichlet Allocation(supvised topic model). The attached is the poster of this project. This browser does not support PDFs. Please download the PDF to view it: Download PDF.]]></content>
      <categories>
        <category>My Teamwork Projects</category>
      </categories>
  </entry>
</search>
